## TransactionsPipeline

**Цел на проектот**

Модерните системи за обработка на финансиски податоци се повеќе се развиваат кон реално-временски архитектури кои динамички ги обработуваат и анализираат трансакциите во тек. Ова истражување има за цел да ја анализира и демонстрира ефикасноста на end-to-end ETL pipeline за финансиски трансакции, систем кој комбинира реално-временска обработка преку Kafka, data lake архитектура (Bronze/Silver/Gold слоеви), и автоматизирана ML-базирана категоризација со примена на техники од вештачка интелигенција. Оваа работа придонесува кон развој на нови пристапи во обработката на финансиски податоци, овозможувајќи скалабилна и инкрементална обработка на големи обеми на трансакциски податоци.

- **Цел на проектот**: Демонстрациски `end-to-end` pipeline за финансиски трансакции, кој користи Kafka, MinIO (S3), DuckDB и Airflow.
- **Bronze слој**: Реално-временски консумер од Kafka (`bronze_write.py`) кој ги запишува суровите трансакции како Parquet фајлови во MinIO (`bronze/`).
- **Silver слој**: Скрипта (`bronze_to_silver.py`) која ги чисти податоците, ги кастира типовите и ги филтрира невалидните редови, па ги запишува „чистите“ трансакции во `silver/`.
- **Gold слој и DW**: Скрипта (`silver_to_gold.py`) која креира димензиони табели и fact табела (data warehouse модел) и ги запишува резултатите во `gold/` како Parquet.
- **Автоматска категоризација**: Модулот `category_classifier_fast.py` користи брз ML/embeddings модел (или оптимизиран keyword match) за да ги категоризира трансакциите (на пр. `groceries`, `restaurants`, `travel`).
- **Оркестрација**: Airflow DAG (`dags/etl_pipeline.py`) ја оркестрира ETL логиката од Bronze → Silver → Gold, со можност за инкрементално процесирање.
- **Инфраструктура**: `docker-compose.yaml` и `Dockerfile.airflow` овозможуваат локално подигање на Kafka, MinIO, Airflow и DuckDB околина за лесно тестирање.
